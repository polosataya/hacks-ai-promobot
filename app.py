import hydralit as hy
import streamlit as st
import pandas as pd
import base64
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import joblib
import emoji
import re
import nltk
import spacy
from spacy import displacy

st.set_page_config( page_title="–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—Ä–∞—â–µ–Ω–∏–π –≥—Ä–∞–∂–¥–∞–Ω",
                    page_icon="ü§ñ", layout="wide", initial_sidebar_state="expanded",
                    menu_items={'Get Help': None,'Report a bug': None,'About': None})

hide_streamlit_style = """<style>#MainMenu {visibility: hidden;}footer {visibility: hidden;}</style>"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

#============================================================================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
#============================================================================

pathModel="model/"

@st.cache_resource
def load_vectorizer(filename):
    '''–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞'''
    tfidf = TfidfVectorizer()
    tfidf = joblib.load(pathModel+filename)
    return tfidf

@st.cache_resource
def load_logreg(filename):
    '''–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞'''
    logit = LogisticRegression()
    logit = joblib.load(pathModel+filename)
    return logit

@st.cache_data
def load_stopwords():
    ''' –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ '''
    nltk.download('stopwords')
    stopwords_nltk = nltk.corpus.stopwords.words('russian') #–ª–∏—Å—Ç —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
    stopwords_nltk_en = nltk.corpus.stopwords.words('english')
    stopwords_nltk.extend(stopwords_nltk_en) #—á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
    new_stop = ['–∑–¥—Ä–∞–≤—Å—Ç–≤–æ–≤–∞—Ç—å', '–ø–æ–¥—Å–∫–∞–∑–∞—Ç—å', '—Å–∫–∞–∑–∞—Ç—å', "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "—Å–ø–∞—Å–∏–±–æ",  "–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å", "–∏–∑–≤–∏–Ω–∏—Ç—å",
                '–≤–æ–ø—Ä–æ—Å', '—Ç–µ–º–∞', "–æ—Ç–≤–µ—Ç", "–æ—Ç–≤–µ—Ç–∏—Ç—å", "–ø–æ—á–µ–º—É", "—á—Ç–æ",
                '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–π', '–∫–æ—Ç–æ—Ä—É—é', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã—Ö', '—ç—Ç–æ', "–º–æ—á—å",
                '–≤–æ–æ–±—â–µ', "–≤—Å—ë", "–≤–µ—Å—å", "–µ—â—ë", "–ø—Ä–æ—Å—Ç–æ",  "—è–∫–æ–±—ã", "–ø—Ä–∏—á—ë–º", '—Ç–æ—á–Ω–æ', "—Ö–æ—Ç—è", "–∏–º–µ–Ω–Ω–æ", '–Ω–µ—É–∂–µ–ª–∏',
                "–≥", "—É–ª", "–≥–æ—Ä–æ–¥", "—É–ª–∏—Ü–∞"]  #—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
    stopwords_nltk.extend(new_stop)
    return stopwords_nltk
@st.cache_resource
def load_nlp():
    nlp = spacy.load('ru_core_news_md')
    return nlp

@st.cache_data
def load_upload(uploaded_file):
    try:
        data = pd.read_csv(uploaded_file, sep=',')
    except:
        data = pd.read_csv(uploaded_file, sep=';')
    return data

#============================================================================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Ç–æ–º
#============================================================================
def full_clean(text):
    '''–æ—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞'''
    text = emoji.demojize(text)
    text=re.sub(r"[^a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9#]", " ", text)
    text = text.lower()
    text = re.sub(" +", " ", text) #–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 1 –ø—Ä–æ–±–µ–ª
    text = text.replace("–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "").replace("–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä", "").replace("–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "").replace("—Å–æ–æ–±—â–µ–Ω–∏–µ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞", "").replace("–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é", "").replace("–¥–æ —Å–∏—Ö –ø–æ—Ä", "")
    text = text.strip()
    #—Ç–æ–∫–µ–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π
    tokens = [token.lemma_ for token in nlp(text) if token.lemma_ not in stopwords_nltk]
    #–¥–ª—è tfidf –Ω–∞ –≤—Ö–æ–¥ —Ç–µ–∫—Å—Ç
    text = " ".join(tokens)
    return text, tokens

def preprocess_text(df):
    '''–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∫ –ø–æ–¥–∞—á–µ –≤ –º–æ–¥–µ–ª—å –∫–æ–ª–æ–Ω–∫–æ–π'''
    new_corpus = []
    new_tokens = []

    for text in df:
        text, tokens = full_clean(text)
        new_corpus.append(text)
        new_tokens.append(tokens)
    return new_corpus, new_tokens

def tfidf_embeding(model=None, df=None):
    '''–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –º–µ—à–æ–∫ —Å–ª–æ–≤'''
    X = model.transform(df)
    return X.toarray()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
def classify_data(logit, data):
    ''' –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è '''
    return logit.predict(data)

#============================================================================
# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
#============================================================================

tfidf = load_vectorizer('tfidf.pkl')
logit_group = load_logreg("logit_group.sav")
logit_title = load_logreg("logit_title.sav")
logit_otdel = load_logreg("logit_otdel.sav")
stopwords_nltk = load_stopwords()
nlp = load_nlp()

#============================================================================
# –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
#============================================================================

# –°–æ–∑–¥–∞–µ–º –≥–ª–∞–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Hydralit
app = hy.HydraApp(title='–ú–Ω–æ–≥–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Streamlit')

# –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –≤–≤–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
@app.addapp('–í–≤–æ–¥')
def input_and_classify_page():
    st.title("–í–≤–æ–¥ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")

    # –ü–æ–ª–µ –≤–≤–æ–¥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
    text_input = st.text_area('–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏')

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    if st.button('–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å'):
        # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä
        text_clean, _ = full_clean(text_input)
        tfidf_embed=tfidf.transform([text_clean])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã —Ç–µ–º
        predict_group = classify_data(logit_group, tfidf_embed)[0]
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º—ã
        predict_title = classify_data(logit_title, tfidf_embed)[0]
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è
        predict_otdel = classify_data(logit_otdel, tfidf_embed)[0]

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")

        # –í—ã–≤–æ–¥ –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏ CSV —Ñ–∞–π–ª–∞
        st.write(f"–ì—Ä—É–ø–ø–∞ —Ç–µ–º: {predict_group}")
        st.write(f"–¢–µ–º–∞: {predict_title}")
        st.write(f" ")
        st.write(f"–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å: {predict_otdel}")

        # NER
        st.subheader("–¢–µ–∫—Å—Ç —Å –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏:")
        doc = nlp(text_input)
        ent_html = displacy.render(doc, style="ent", jupyter = False)

        st.write(f" ")
        # Display the entity visualization in the browser:
        st.markdown(ent_html, unsafe_allow_html=True)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
@app.addapp('–ó–∞–≥—Ä—É–∑–∫–∞')
def upload_and_display_page():
    st.title("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–Ω–æ–ø–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏", type=["csv"])

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if uploaded_file is not None and st.button('–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å'):
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
        uploaded_data = load_upload(uploaded_file)

        uploaded_data['text_clean'], uploaded_data['tokens'] = preprocess_text(uploaded_data['–¢–µ–∫—Å—Ç –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞'])
        tfidf_embed = tfidf_embeding(model=tfidf, df=uploaded_data['text_clean'])

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã —Ç–µ–º
        predict_group = classify_data(logit_group, tfidf_embed)
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º—ã
        predict_title = classify_data(logit_title, tfidf_embed)
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è
        predict_otdel = classify_data(logit_otdel, tfidf_embed)

        #–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        uploaded_data["–ì—Ä—É–ø–ø–∞ —Ç–µ–º"]=predict_group
        uploaded_data["–¢–µ–º–∞"] = predict_title
        uploaded_data["–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å"] = predict_otdel

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—Å–µ–π —Ç–∞–±–ª–∏—Ü—ã
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        st.dataframe(uploaded_data[["–¢–µ–∫—Å—Ç –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞", "–ì—Ä—É–ø–ø–∞ —Ç–µ–º", "–¢–µ–º–∞", "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å"]])

        # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        csv = uploaded_data[["–¢–µ–∫—Å—Ç –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞", "–ì—Ä—É–ø–ø–∞ —Ç–µ–º", "–¢–µ–º–∞", "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å"]].to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="result.csv"><button>–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç</button></a>'
        st.markdown(href, unsafe_allow_html=True)

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Hydralit
app.run()